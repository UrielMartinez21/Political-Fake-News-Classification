{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# Below is the code used to train and test the model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Installation of the required modules\n","!pip install torch                    # Install the PyTorch library for deep learning.\n","!pip install --upgrade transformers  # Install and upgrade the Transformers library for NLP tasks.\n","!pip install pandas                  # Install the Pandas library for data manipulation.\n","!pip install scikit-learn            # Install scikit-learn for machine learning tasks.\n","!pip install sentencepiece           # Install SentencePiece for text tokenization.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Train the model with the training dataset."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# --------------------------------------| Preparaci贸n del entorno y carga de datos |-------------------------------------- #\n","# Bibliotecas necesarias\n","import torch\n","import pandas as pd\n","import random\n","import numpy as np\n","from transformers import RobertaForSequenceClassification, RobertaTokenizer, AdamW\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, matthews_corrcoef\n","\n","# Esto es para que los resultados sean reproducibles\n","seed = 26                                           # La semilla que queramos\n","random.seed(seed)                                   # Fijamos la semilla para random\n","np.random.seed(seed)                                # Fijamos la semilla para numpy (usado por pandas)\n","torch.manual_seed(seed)                             # Fijamos la semilla para torch\n","torch.cuda.manual_seed_all(seed)                    # Fijamos la semilla para cuda (GPU)\n","\n","# Cargar el dataset de entrenamiento\n","df = pd.read_csv(\"/content/drive/MyDrive/DATASET/D46000_train.csv\", delimiter=\";\")\n","\n","# Separar rasgos y etiquetas\n","X = df[[\"Titulo\", \"Descripcion\", \"Fecha\"]]\n","y = df[\"Label\"]\n","\n","# Separar la informacion en conjuntos de entrenamiento y evaluacion usando validacion cruzada estratificada\n","skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)   # Definir el generador de validacion cruzada estratificada\n","train_indices, eval_indices = next(skf.split(X, y))                 # Obtener los indices de los conjuntos de entrenamiento y evaluacion\n","\n","X_train, X_eval = X.iloc[train_indices], X.iloc[eval_indices]       # Obtener los conjuntos de entrenamiento y evaluacion\n","y_train, y_eval = y.iloc[train_indices], y.iloc[eval_indices]       # Obtener las etiquetas de los conjuntos de entrenamiento y evaluacion\n","\n","# --------------------------------------| Tokenizaci贸n y preparaci贸n de datos |-------------------------------------- #\n","# Tokenizador de Roberta\n","tokenizer = RobertaTokenizer.from_pretrained(\"PlanTL-GOB-ES/roberta-base-bne\")\n","\n","# Codificar el conjunto de entrenamiento\n","train_encodings = tokenizer(\n","    X_train[\"Titulo\"].tolist(),             # Lista de titulos\n","    X_train[\"Descripcion\"].tolist(),        # Lista de descripciones\n","    X_train[\"Fecha\"].tolist(),              # Lista de fechas\n","    padding=\"max_length\",                   # Rellenar las secuencias hasta la longitud maxima\n","    truncation='only_second',               # Si la noticia excede la longitud maxima, se truncara el segundo campo\n","    max_length=128,                         # Longitud maxima de las secuencias\n","    return_tensors=\"pt\"                     # Devolver los tensores de PyTorch\n",")\n","# Codificar el conjunto de evaluacion\n","eval_encodings = tokenizer(                 # Mismo procedimiento que para el conjunto de entrenamiento\n","    X_eval[\"Titulo\"].tolist(),\n","    X_eval[\"Descripcion\"].tolist(),\n","    X_eval[\"Fecha\"].tolist(),\n","    padding=\"max_length\",\n","    truncation='only_second',               # Si la noticia excede la longitud maxima, se truncara el segundo campo\n","    max_length=128,\n","    return_tensors=\"pt\"\n",")\n","\n","# Asignar las entradas codificadas a variables separadas\n","train_input_ids = train_encodings[\"input_ids\"]\n","train_attention_masks = train_encodings[\"attention_mask\"]\n","\n","eval_input_ids = eval_encodings[\"input_ids\"]\n","eval_attention_masks = eval_encodings[\"attention_mask\"]\n","\n","# Crear TensorDatasets para entrenamiento y evaluacion\n","train_dataset = TensorDataset(train_input_ids, train_attention_masks, torch.tensor(y_train.tolist()))\n","eval_dataset = TensorDataset(eval_input_ids, eval_attention_masks, torch.tensor(y_eval.tolist()))\n","\n","# Crear DataLoaders para cargar los datos en lotes\n","batch_size = 16\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n","\n","# --------------------------------------| Configuraci贸n del modelo y entrenamiento |-------------------------------------- #\n","# Cargar el modelo pre-entrenado\n","model = RobertaForSequenceClassification.from_pretrained(\"PlanTL-GOB-ES/roberta-base-bne\", num_labels=2)\n","\n","# Configurar el optimizador y el dispositivo de entrenamiento\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)  # Add weight decay for L2 regularization 0.1\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","# Configurar la tasa de dropout\n","dropout_rate = 0.1                              # Esto para que sea facil cambiarlo\n","model.classifier.dropout.p = dropout_rate       # Configurar la tasa de dropout para la capa de clasificacion\n","\n","# Modelo de entrenamiento\n","model.train()\n","\n","# Variables para guardar el mejor modelo\n","best_mcc = -1.0                                 # Best MCC value\n","best_epoch = -1                                 # Epoch where the best MCC was achieved\n","best_model_path = \"/content/drive/MyDrive/DATASET/YourPath\"  # Path to save the best model\n","\n","for epoch in range(10):\n","    total_train_loss = 0.0\n","    correct_predictions = 0\n","    total_predictions = 0\n","\n","    for batch in train_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        input_ids, attention_masks, labels = batch\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n","        loss = outputs.loss\n","\n","        # Add L2 regularization penalty\n","        #l2_lambda = 0.1  # L2 regularization factor 0.1\n","        #l2_reg = torch.tensor(0., device=device)\n","        #for param in model.parameters():\n","        #    l2_reg += torch.norm(param, p=2)\n","        #loss += l2_lambda * l2_reg\n","\n","        logits = outputs.logits\n","\n","        total_train_loss += loss.item()\n","\n","        _, predicted_labels = torch.max(logits, 1)\n","        correct_predictions += (predicted_labels == labels).sum().item()\n","        total_predictions += labels.size(0)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","    train_loss = total_train_loss / len(train_dataloader)\n","    train_accuracy = correct_predictions / total_predictions\n","\n","    # Evaluation on the evaluation set\n","    model.eval()\n","\n","    with torch.no_grad():\n","        total_eval_loss = 0.0\n","        eval_predictions = []\n","        eval_labels = []\n","\n","        for batch in eval_dataloader:\n","            batch = tuple(t.to(device) for t in batch)\n","            input_ids, attention_masks, labels = batch\n","\n","            outputs = model(input_ids, attention_mask=attention_masks, labels=labels)\n","            loss = outputs.loss\n","            logits = outputs.logits\n","\n","            total_eval_loss += loss.item()\n","\n","            _, predicted_labels = torch.max(logits, 1)\n","            eval_predictions.extend(predicted_labels.tolist())\n","            eval_labels.extend(labels.tolist())\n","\n","        eval_loss = total_eval_loss / len(eval_dataloader)\n","        eval_accuracy = accuracy_score(eval_labels, eval_predictions)\n","        eval_f1 = f1_score(eval_labels, eval_predictions)\n","        eval_recall = recall_score(eval_labels, eval_predictions)\n","        eval_mcc = matthews_corrcoef(eval_labels, eval_predictions)\n","\n","    print(f\"Epoca {epoch + 1}\")\n","    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n","    print(f\"Eval Loss: {eval_loss:.4f} | Eval Accuracy: {eval_accuracy:.4f}\")\n","    print(f\"Eval F1: {eval_f1:.4f}\")\n","    print(f\"Eval Recall: {eval_recall:.4f}\")\n","    print(f\"Eval MCC: {eval_mcc:.4f}\")\n","    print(\"--------------------\")\n","\n","    # --------------------------------------| Guardado del mejor modelo |-------------------------------------- #\n","    # Guardar el modelo si se consigue un MCC mayor\n","    if eval_mcc > best_mcc:\n","        model.save_pretrained(best_model_path)\n","        tokenizer.save_pretrained(best_model_path)\n","        best_mcc = eval_mcc\n","        best_epoch = epoch + 1\n","\n","# --------------------------------------| Resultados finales |-------------------------------------- #\n","print(\"Best model achieved at epoch:\", best_epoch)\n","print(\"Best evaluation MCC:\", best_mcc)\n","print(\"Model saved at:\", best_model_path)\n"]},{"cell_type":"markdown","metadata":{},"source":["# Test the model with unseen data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from transformers import RobertaTokenizer, RobertaForSequenceClassification\n","from torch.utils.data import DataLoader, TensorDataset\n","import pandas as pd\n","from sklearn.metrics import accuracy_score, f1_score, recall_score, matthews_corrcoef\n","import random\n","import numpy as np\n","\n","# Set the seed for reproducibility\n","seed = 26\n","random.seed(seed)\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","torch.cuda.manual_seed_all(seed)\n","\n","# Define the paths to the pre-trained model and tokenizer\n","model_path = \"/content/drive/MyDrive/DATASET/YourSavedModel\"\n","tokenizer_path = \"/content/drive/MyDrive/DATASET/YourSavedModel\"\n","\n","# Load the pre-trained model and tokenizer\n","model = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=2)\n","tokenizer = RobertaTokenizer.from_pretrained(tokenizer_path)\n","\n","# Load new data from a CSV file\n","new_data_path = \"/content/drive/MyDrive/DATASET/D11000_test.csv\"\n","new_df = pd.read_csv(new_data_path, delimiter=';')\n","\n","# Set batch size for inference\n","batch_size = 16\n","\n","# Tokenize the new data\n","new_encodings = tokenizer(\n","    new_df[\"Titulo\"].tolist(),\n","    new_df[\"Descripcion\"].tolist(),\n","    new_df[\"Fecha\"].tolist(),\n","    padding=\"max_length\",\n","    truncation='only_second',\n","    max_length=128,\n","    return_tensors=\"pt\"\n",")\n","\n","new_input_ids = new_encodings[\"input_ids\"]\n","new_attention_masks = new_encodings[\"attention_mask\"]\n","\n","# Create a TensorDataset and DataLoader for the new data\n","new_dataset = TensorDataset(new_input_ids, new_attention_masks)\n","new_dataloader = DataLoader(new_dataset, batch_size=batch_size, shuffle=False)\n","\n","# Use GPU if available, otherwise use CPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Set the model to evaluation mode\n","model.eval()\n","\n","# Perform inference on the new data\n","predictions = []\n","with torch.no_grad():\n","    for batch in new_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        input_ids, attention_masks = batch\n","\n","        outputs = model(input_ids, attention_mask=attention_masks)\n","        logits = outputs.logits\n","\n","        _, predicted_labels = torch.max(logits, 1)\n","        predictions.extend(predicted_labels.tolist())\n","\n","# Get true labels from the new data\n","true_labels = new_df[\"Label\"].tolist()\n","\n","# Calculate and print evaluation metrics\n","accuracy = accuracy_score(true_labels, predictions)\n","f1 = f1_score(true_labels, predictions)\n","recall = recall_score(true_labels, predictions)\n","mcc = matthews_corrcoef(true_labels, predictions)\n","\n","print(f\"Accuracy: {accuracy:.4f}\")\n","print(f\"F1 Score: {f1:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"MCC: {mcc:.4f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["# Example of results:\n","Epoch 1\n","\n","Train Loss: 0.1155 | Train Accuracy: 0.9538\n","\n","Eval Loss: 0.0890 | Eval Accuracy: 0.9674\n","\n","Eval F1: 0.9712\n","\n","Eval Recall: 0.9494\n","\n","Eval MCC: 0.9350\n","\n","--------------------\n","Epoch 2\n","\n","Train Loss: 0.0276 | Train Accuracy: 0.9910\n","\n","Eval Loss: 0.0549 | Eval Accuracy: 0.9822\n","\n","Eval F1: 0.9847\n","\n","Eval Recall: 0.9865\n","\n","Eval MCC: 0.9634\n","\n","--------------------\n","Epoch 3\n","\n","Train Loss: 0.0153 | Train Accuracy: 0.9956\n","\n","Eval Loss: 0.0545 | Eval Accuracy: 0.9826\n","\n","Eval F1: 0.9850\n","\n","Eval Recall: 0.9824\n","\n","Eval MCC: 0.9644\n","\n","--------------------\n","Epoch 4\n","\n","Train Loss: 0.0116 | Train Accuracy: 0.9965\n","\n","Eval Loss: 0.0532 | Eval Accuracy: 0.9830\n","\n","Eval F1: 0.9854\n","\n","Eval Recall: 0.9835\n","\n","Eval MCC: 0.9652\n","\n","--------------------\n","Epoch 5\n","\n","Train Loss: 0.0064 | Train Accuracy: 0.9980\n","\n","Eval Loss: 0.0735 | Eval Accuracy: 0.9826\n","\n","Eval F1: 0.9850\n","\n","Eval Recall: 0.9824\n","\n","Eval MCC: 0.9644\n","\n","--------------------\n","Mejor modelo alcanzado en la 茅poca: 4\n","\n","Mejor MCC de evaluaci贸n: 0.9652303984884018\n","\n","Modelo guardado en: /content/drive/MyDrive/DATASET/D46000_BNE_batch16_L20_drop05\n","\n","**TEST**\n","\n","Accuracy: 0.9857\n","\n","F1 Score: 0.9880\n","\n","Recall: 0.9901\n","\n","MCC: 0.9703\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
